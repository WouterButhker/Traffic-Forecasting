{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: K120.csv\n",
      "Finished reading file: K120.csv, shape = (2880, 7)\n",
      "Reading file: K134.csv\n",
      "Finished reading file: K134.csv, shape = (2880, 7)\n",
      "Reading file: K140.csv\n",
      "Finished reading file: K140.csv, shape = (2880, 5)\n",
      "Reading file: K159.csv\n",
      "Finished reading file: K159.csv, shape = (2880, 11)\n",
      "Reading file: K405.csv\n",
      "Finished reading file: K405.csv, shape = (2880, 19)\n",
      "Reading file: K406.csv\n",
      "Finished reading file: K406.csv, shape = (2880, 8)\n",
      "Reading file: K701.csv\n",
      "Finished reading file: K701.csv, shape = (2880, 7)\n",
      "Reading file: K702.csv\n",
      "Finished reading file: K702.csv, shape = (2880, 8)\n",
      "Reading file: K703.csv\n",
      "Finished reading file: K703.csv, shape = (2880, 10)\n",
      "Reading file: K709.csv\n",
      "Finished reading file: K709.csv, shape = (2880, 17)\n",
      "Reading file: K711.csv\n",
      "Finished reading file: K711.csv, shape = (2880, 31)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data_cars/'\n",
    "all_files = os.listdir(DATA_PATH)\n",
    "\n",
    "all_dataframes = []\n",
    "for index, file in enumerate(all_files):\n",
    "    print(f\"Reading file: {file}\")\n",
    "    file_name = file.split('.')[0]\n",
    "    df = pd.read_csv(DATA_PATH + file, sep=';')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df[file_name], format='%Y-%m-%d %H:%M')\n",
    "    df = df.drop(columns=[file_name])\n",
    "\n",
    "    df = df.set_index('date')\n",
    "    df.columns = [f\"{file_name}_{col}\" for col in df.columns if col != 'date']\n",
    "    all_dataframes.append(df)\n",
    "    print(f\"Finished reading file: {file}, shape = {df.shape}\")\n",
    "\n",
    "combined_df = pd.concat(all_dataframes, axis=1)\n",
    "combined_df.fillna(method='ffill', inplace=True)\n",
    "combined_df['hour'] = combined_df.index.hour\n",
    "combined_df['day_of_week'] = combined_df.index.dayofweek\n",
    "\n",
    "data = np.array(combined_df, dtype=float)[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSequence(seq, n_steps):\n",
    "\n",
    "    #Declare X and y as empty list\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        #get the last index\n",
    "        lastIndex = i + n_steps\n",
    "\n",
    "        #if lastIndex is greater than length of sequence then break\n",
    "        if lastIndex > len(seq) - 1:\n",
    "            break\n",
    "\n",
    "        # Create input and output sequence\n",
    "        # Last 2 columns are time of day and day of week\n",
    "        seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n",
    "\n",
    "        #append seq_X, seq_y in X and y list\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        #Convert X and y into numpy array\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftSequence(seq, n_steps):\n",
    "\n",
    "    #Declare X and y as empty list\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        #get the last index\n",
    "        lastIndex = i + n_steps\n",
    "\n",
    "        #if lastIndex is greater than length of sequence then break\n",
    "        if lastIndex > len(seq) - 1:\n",
    "            break\n",
    "\n",
    "        # Create input and output sequence\n",
    "        # Last 2 columns are time of day and day of week\n",
    "        seq_X, seq_y = seq[i:lastIndex], seq[i+1:lastIndex+1]\n",
    "\n",
    "        #append seq_X, seq_y in X and y list\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        #Convert X and y into numpy array\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 80)\n",
      "(1728, 80)\n"
     ]
    }
   ],
   "source": [
    "num_of_steps = data.shape[0]\n",
    "train_size = 0.6\n",
    "val_size = 0.15\n",
    "shuffle = False\n",
    "look_back = 80\n",
    "\n",
    "x, y = splitSequence(data, look_back)\n",
    "\n",
    "x_int, y_int = splitSequence(data[:, :7], look_back)\n",
    "\n",
    "if shuffle:\n",
    "    idx = np.random.permutation(len(x))\n",
    "    x,y = x[idx], y[idx]\n",
    "\n",
    "num_train = int(num_of_steps * train_size)\n",
    "num_val = int(num_of_steps * val_size)\n",
    "\n",
    "x_train, y_train = x[:num_train], y[:num_train]\n",
    "x_val, y_val = x[num_train:num_train + num_val], y[num_train:num_train + num_val]\n",
    "x_test, y_test = x[num_train + num_val:], y[num_train + num_val:]\n",
    "\n",
    "x_train_singleInt, y_train_singleInt = x_int[:num_train], y_int[:num_train]\n",
    "x_val_singleInt, y_val_singleInt = x_int[num_train:num_train + num_val], y_int[num_train:num_train + num_val]\n",
    "x_test_singleInt, y_test_singleInt = x_int[num_train + num_val:], y_int[num_train + num_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 80) (2800, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  4.,  0.,  0.,  1.,  4.,  4.,  1.,  0.,  1.,  1.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  4.,  1.,\n",
       "         4.,  5.,  6.,  7., 13., 21., 18., 43., 42., 26., 20., 17., 18.,\n",
       "        21., 31., 16., 24., 20., 30., 21., 22., 69., 38., 26., 41., 25.,\n",
       "        25., 24., 33., 33., 32., 34., 63., 61., 48., 55., 34., 39., 28.,\n",
       "        36., 52., 30., 35., 62., 37., 25., 15., 20., 17., 16.,  7.,  8.,\n",
       "        16., 11.]),\n",
       " array([ 4.,  0.,  0.,  1.,  4.,  4.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  4.,  1.,  4.,\n",
       "         5.,  6.,  7., 13., 21., 18., 43., 42., 26., 20., 17., 18., 21.,\n",
       "        31., 16., 24., 20., 30., 21., 22., 69., 38., 26., 41., 25., 25.,\n",
       "        24., 33., 33., 32., 34., 63., 61., 48., 55., 34., 39., 28., 36.,\n",
       "        52., 30., 35., 62., 37., 25., 15., 20., 17., 16.,  7.,  8., 16.,\n",
       "        11., 12.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one, y_one = shiftSequence(data[:, 0], look_back)\n",
    "print(x_one.shape, y_one.shape)\n",
    "x_one[0], y_one[0]\n",
    "\n",
    "x_train_single, y_train_single = x_one[:num_train], y_one[:num_train]\n",
    "x_val_single, y_val_single = x_one[num_train:num_train + num_val], y_one[num_train:num_train + num_val]\n",
    "x_test_single, y_test_single = x_one[num_train + num_val:], y_one[num_train + num_val:]\n",
    "\n",
    "print(y_train_single.shape)\n",
    "print(x_train_single.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75016484]\n",
      " [-0.66650333]\n",
      " [-1.00114935]]\n",
      "[0.00278872]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_single[0, :3])\n",
    "print(y_train_single[0, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "#Following structure layed out in github.com/locuslab/TCN\n",
    "def createTemporalConvNetwork(n_layers, n_sensors, look_back, n_outputs, kernel_size=2, dropout=0.2):\n",
    "    modelTCN = keras.models.Sequential()\n",
    "    modelTCN.add(layers.InputLayer((look_back, n_sensors)))\n",
    "    for i in range(n_layers):\n",
    "        modelTCN.add(layers.Conv1D(128, kernel_size=kernel_size, padding='causal', activation='relu', dilation_rate=2**i))\n",
    "        modelTCN.add(layers.Dropout(dropout))\n",
    "        modelTCN.add(layers.Conv1D(128, kernel_size=kernel_size, padding='causal', activation='relu', dilation_rate=2**i))\n",
    "        modelTCN.add(layers.Dropout(dropout))\n",
    "    modelTCN.add(layers.Conv1D(n_outputs, kernel_size=kernel_size, padding='causal', activation='relu', dilation_rate=2**(i+1)))\n",
    "    return modelTCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_107 (Conv1D)         (None, 80, 128)           384       \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_108 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_109 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_110 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_111 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_100 (Dropout)       (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_112 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_113 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_114 (Conv1D)         (None, 80, 128)           32896     \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, 80, 128)           0         \n",
      "                                                                 \n",
      " conv1d_115 (Conv1D)         (None, 80, 1)             257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 230,913\n",
      "Trainable params: 230,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "54/54 [==============================] - 5s 71ms/step - loss: 98.8562 - root_mean_squared_error: 9.9426 - val_loss: 108.2189 - val_root_mean_squared_error: 10.4028\n",
      "Epoch 2/500\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 59.9997 - root_mean_squared_error: 7.7459 - val_loss: 107.7031 - val_root_mean_squared_error: 10.3780\n",
      "Epoch 3/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 55.6415 - root_mean_squared_error: 7.4593 - val_loss: 123.3745 - val_root_mean_squared_error: 11.1074\n",
      "Epoch 4/500\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 52.6833 - root_mean_squared_error: 7.2583 - val_loss: 125.9805 - val_root_mean_squared_error: 11.2241\n",
      "Epoch 5/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 51.1205 - root_mean_squared_error: 7.1499 - val_loss: 109.8531 - val_root_mean_squared_error: 10.4811\n",
      "Epoch 6/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 49.9863 - root_mean_squared_error: 7.0701 - val_loss: 117.4926 - val_root_mean_squared_error: 10.8394\n",
      "Epoch 7/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 49.3566 - root_mean_squared_error: 7.0254 - val_loss: 101.2941 - val_root_mean_squared_error: 10.0645\n",
      "Epoch 8/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 47.8722 - root_mean_squared_error: 6.9190 - val_loss: 106.8274 - val_root_mean_squared_error: 10.3357\n",
      "Epoch 9/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 46.7420 - root_mean_squared_error: 6.8368 - val_loss: 104.2321 - val_root_mean_squared_error: 10.2094\n",
      "Epoch 10/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 46.2797 - root_mean_squared_error: 6.8029 - val_loss: 105.0409 - val_root_mean_squared_error: 10.2489\n",
      "Epoch 11/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 45.4498 - root_mean_squared_error: 6.7416 - val_loss: 104.8462 - val_root_mean_squared_error: 10.2394\n",
      "Epoch 12/500\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 44.0222 - root_mean_squared_error: 6.6349 - val_loss: 93.9306 - val_root_mean_squared_error: 9.6918\n",
      "Epoch 13/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 43.2479 - root_mean_squared_error: 6.5763 - val_loss: 92.9623 - val_root_mean_squared_error: 9.6417\n",
      "Epoch 14/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 41.7444 - root_mean_squared_error: 6.4610 - val_loss: 83.1076 - val_root_mean_squared_error: 9.1163\n",
      "Epoch 15/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 40.6052 - root_mean_squared_error: 6.3722 - val_loss: 89.2592 - val_root_mean_squared_error: 9.4477\n",
      "Epoch 16/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 38.8186 - root_mean_squared_error: 6.2305 - val_loss: 83.8231 - val_root_mean_squared_error: 9.1555\n",
      "Epoch 17/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 36.8162 - root_mean_squared_error: 6.0676 - val_loss: 78.3677 - val_root_mean_squared_error: 8.8526\n",
      "Epoch 18/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 35.3553 - root_mean_squared_error: 5.9460 - val_loss: 87.3532 - val_root_mean_squared_error: 9.3463\n",
      "Epoch 19/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 33.8845 - root_mean_squared_error: 5.8210 - val_loss: 77.7378 - val_root_mean_squared_error: 8.8169\n",
      "Epoch 20/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 32.1855 - root_mean_squared_error: 5.6732 - val_loss: 78.1772 - val_root_mean_squared_error: 8.8418\n",
      "Epoch 21/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 30.8957 - root_mean_squared_error: 5.5584 - val_loss: 75.3464 - val_root_mean_squared_error: 8.6802\n",
      "Epoch 22/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 29.7649 - root_mean_squared_error: 5.4557 - val_loss: 71.9357 - val_root_mean_squared_error: 8.4815\n",
      "Epoch 23/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 28.8614 - root_mean_squared_error: 5.3723 - val_loss: 73.7657 - val_root_mean_squared_error: 8.5887\n",
      "Epoch 24/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 27.3501 - root_mean_squared_error: 5.2297 - val_loss: 72.6940 - val_root_mean_squared_error: 8.5261\n",
      "Epoch 25/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 26.2948 - root_mean_squared_error: 5.1278 - val_loss: 71.1179 - val_root_mean_squared_error: 8.4331\n",
      "Epoch 26/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 25.3648 - root_mean_squared_error: 5.0363 - val_loss: 71.0767 - val_root_mean_squared_error: 8.4307\n",
      "Epoch 27/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 24.5485 - root_mean_squared_error: 4.9546 - val_loss: 68.1325 - val_root_mean_squared_error: 8.2542\n",
      "Epoch 28/500\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 23.8412 - root_mean_squared_error: 4.8827 - val_loss: 72.4158 - val_root_mean_squared_error: 8.5097\n",
      "Epoch 29/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 23.4618 - root_mean_squared_error: 4.8437 - val_loss: 68.9060 - val_root_mean_squared_error: 8.3010\n",
      "Epoch 30/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 22.6400 - root_mean_squared_error: 4.7582 - val_loss: 67.1546 - val_root_mean_squared_error: 8.1948\n",
      "Epoch 31/500\n",
      "54/54 [==============================] - 4s 71ms/step - loss: 21.8147 - root_mean_squared_error: 4.6706 - val_loss: 61.6728 - val_root_mean_squared_error: 7.8532\n",
      "Epoch 32/500\n",
      "54/54 [==============================] - 4s 76ms/step - loss: 21.8072 - root_mean_squared_error: 4.6698 - val_loss: 61.6976 - val_root_mean_squared_error: 7.8548\n",
      "Epoch 33/500\n",
      "54/54 [==============================] - 4s 74ms/step - loss: 21.2174 - root_mean_squared_error: 4.6062 - val_loss: 67.5786 - val_root_mean_squared_error: 8.2206\n",
      "Epoch 34/500\n",
      "54/54 [==============================] - 4s 71ms/step - loss: 20.6524 - root_mean_squared_error: 4.5445 - val_loss: 68.0487 - val_root_mean_squared_error: 8.2492\n",
      "Epoch 35/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 20.2615 - root_mean_squared_error: 4.5013 - val_loss: 66.0644 - val_root_mean_squared_error: 8.1280\n",
      "Epoch 36/500\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 19.9743 - root_mean_squared_error: 4.4693 - val_loss: 67.7884 - val_root_mean_squared_error: 8.2334\n",
      "54/54 [==============================] - 1s 19ms/step\n",
      "20/20 [==============================] - 0s 20ms/step\n",
      "Train Score: 3.20 RMSE\n",
      "Test Score: 7.56 RMSE\n"
     ]
    }
   ],
   "source": [
    "modelCNN = createTemporalConvNetwork(4, 1, look_back, 1, kernel_size=2, dropout=0.3)\n",
    "\n",
    "modelCNN.summary()\n",
    "\n",
    "modelCNN.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "modelCNN.fit(\n",
    "    x=x_train_single,\n",
    "    y=y_train_single,\n",
    "    validation_data=(x_val_single, y_val_single),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    #makes the training stop early if it notices no improvements on the validation set 10 times in a row, to prevent overfitting\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5)],\n",
    ")\n",
    "\n",
    "# make predictions\n",
    "trainPredict = modelCNN.predict(x_train_single)\n",
    "testPredict = modelCNN.predict(x_test_single)\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt((mean_squared_error(y_train_single[:, -1], trainPredict[:, -1])))\n",
    "print(f'Train Score: {trainScore:.2f} RMSE')\n",
    "testScore = np.sqrt(mean_squared_error(y_test_single[:, -1], testPredict[:, -1]))\n",
    "print(f'Test Score: {testScore:.2f} RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(testY, label='actual')\n",
    "plt.plot(testPredict, label='predict')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = createTemporalConvNetwork(4, 7, look_back, 1, kernel_size=2)\n",
    "\n",
    "#modelCNN.summary()\n",
    "\n",
    "modelCNN.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "history = modelCNN.fit(\n",
    "    x=x_train_singleInt,\n",
    "    y=y_train_single,\n",
    "    validation_data=(x_val_singleInt, y_val_single),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    #makes the training stop early if it notices no improvements on the validation set 10 times in a row, to prevent overfitting\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=8)],\n",
    ")\n",
    "\n",
    "# make predictions\n",
    "trainPredict = modelCNN.predict(x_train_singleInt)\n",
    "testPredict = modelCNN.predict(x_test_singleInt)\n",
    "#invert predictions\n",
    "trainPredict = singleScaler.inverse_transform(trainPredict)\n",
    "trainY = singleScaler.inverse_transform(y_train_single)\n",
    "testPredict = singleScaler.inverse_transform(testPredict)\n",
    "testY = singleScaler.inverse_transform(y_test_single)\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(trainY , trainPredict))\n",
    "print(f'Train Score: {trainScore:.2f} RMSE')\n",
    "testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "print(f'Test Score: {testScore:.2f} RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = createTemporalConvNetwork(8, 130, look_back, 130, kernel_size=3)\n",
    "\n",
    "#modelCNN.summary()\n",
    "\n",
    "modelCNN.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "history = modelCNN.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    #makes the training stop early if it notices no improvements on the validation set 10 times in a row, to prevent overfitting\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5)],\n",
    ")\n",
    "\n",
    "# make predictions\n",
    "trainPredict = modelCNN.predict(x_train)\n",
    "testPredict = modelCNN.predict(x_test)\n",
    "#invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(y_train)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(y_test)\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(trainY , trainPredict))\n",
    "print(f'Train Score: {trainScore:.2f} RMSE')\n",
    "testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "print(f'Test Score: {testScore:.2f} RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(testY[:,0], label='actual')\n",
    "plt.plot(testPredict[:,0], label='predict')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostCorrelated(sensor, no_extraSensors, data):\n",
    "    corr = np.corrcoef(data.T)\n",
    "    best = np.argsort(corr[sensor])[-no_extraSensors:]\n",
    "    return data[:, best]\n",
    "\n",
    "def getScore(model, x, y, scaler):\n",
    "    predicts = model.predict(x)\n",
    "    scaled_predict = scaler.inverse_transform(predicts)\n",
    "    return np.sqrt(mean_squared_error(y, scaled_predict))\n",
    "    \n",
    "chosenSensor, no_inputSensors = 0, 10\n",
    "mostCorrelated = getMostCorrelated(chosenSensor, no_inputSensors, data)\n",
    "\n",
    "num_of_steps = data.shape[0] - look_back\n",
    "train_size = 0.6\n",
    "val_size = 0.15\n",
    "num_train = int(num_of_steps * train_size)\n",
    "num_val = int(num_of_steps * val_size)\n",
    "\n",
    "def experiment(no_inputs, no_samples, averaging):\n",
    "    sample_scores = np.zeros((no_samples, len(no_inputs)))\n",
    "    for sample in range(no_samples):\n",
    "        predicted_sensor = np.random.choice(np.arange(data.shape[1]))\n",
    "        _, y = data[:, predicted_sensor]\n",
    "        y_scaler = StandardScaler()\n",
    "        y = y_scaler.fit_transform(y)\n",
    "        y_train, y_val, y_test = y[:num_train], y[num_train:num_val], y[num_train+num_val:]\n",
    "\n",
    "        for idx, inputs in enumerate(no_inputs):\n",
    "            correlated_data = getMostCorrelated(predicted_sensor, inputs, data)\n",
    "            scaler = StandardScaler()\n",
    "            correlated_data = scaler.fit_transform(correlated_data)\n",
    "            x, _ = splitSequence(correlated_data)\n",
    "            x_train, x_val, x_test = x[:num_train], x[num_train:num_val], x[num_train+num_val:]\n",
    "            \n",
    "            scores = np.zeros(averaging)\n",
    "            for av in range(averaging):\n",
    "                model = createTemporalConvNetwork(4, inputs, look_back, 1)\n",
    "                cb = [keras.callbacks.EarlyStopping(patience=10), keras.callbacks.ModelCheckpoint('./checkpoints/', save_best_only=True)]\n",
    "                model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=32, epochs=500, callbacks=cb)\n",
    "                scores[av] = getScore(model, x_test, y_test, y_scaler)\n",
    "            sample_scores[sample, idx] = np.average(scores)\n",
    "    final_scores = np.average(sample_scores, axis=0)\n",
    "    plt.plot(final_scores, no_inputs)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
