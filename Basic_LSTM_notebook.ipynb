{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "df = pd.read_csv('data/K120.csv', delimiter=';').to_numpy()\n",
    "#remove first row of timestamps\n",
    "data = np.array(df[:, 1:], dtype=float)\n",
    "#transform nans to 0 (bandaid solution)\n",
    "data = np.nan_to_num(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_steps = data.shape[0]\n",
    "train_size = 0.5\n",
    "val_size = 0.2\n",
    "\n",
    "num_train = int(num_of_steps * train_size)\n",
    "num_val = int(num_of_steps * val_size)\n",
    "\n",
    "train_set = data[:num_train]\n",
    "mean, std = np.nanmean(train_set, axis=0), np.nanstd(train_set, axis=0)\n",
    "\n",
    "#Will give normalized data as input\n",
    "# train_set = (train_set - mean) / std\n",
    "# val_set = (data[num_train:num_train + num_val] - mean) / std\n",
    "# test_set = (data[num_train + num_val:] - mean) / std\n",
    "\n",
    "#raw data as input\n",
    "train_set = train_set\n",
    "val_set = data[num_train:num_train + num_val]\n",
    "test_set = data[num_train + num_val:]\n",
    "\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import timeseries_dataset_from_array\n",
    "\n",
    "#creates tf.data.Dataset objects which contain tuples of (input, label)\n",
    "#The function timeseries_dataset_from_array transforms the data to a sliding window format and batches it for improved performance\n",
    "#inspired by(directly taken from) https://keras.io/examples/timeseries/timeseries_traffic_forecasting/\n",
    "\n",
    "batch_size = 32\n",
    "input_sequence_length = 8\n",
    "forecast_horizon = 1\n",
    "multi_horizon = False\n",
    "\n",
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        data_array[:-forecast_horizon],\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    target_offset = (\n",
    "        input_sequence_length\n",
    "        if multi_horizon\n",
    "        else input_sequence_length + forecast_horizon - 1\n",
    "    )\n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        data_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "train_dataset, val_dataset = (\n",
    "    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
    "    for data_array in [train_set, val_set]\n",
    ")\n",
    "\n",
    "test_dataset = create_tf_dataset(\n",
    "    test_set,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=test_set.shape[0],\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dataset.as_numpy_iterator())[0][0][1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSequence(seq, n_steps):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(seq)):\n",
    "        #get the last index\n",
    "        lastIndex = i + n_steps\n",
    "\n",
    "        #if lastIndex is greater than length of sequence then break\n",
    "        if lastIndex > len(seq) - 1:\n",
    "            break\n",
    "\n",
    "        #Create input and output sequence\n",
    "        seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n",
    "\n",
    "        #append seq_X, seq_y in X and y list\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        pass    #Convert X and y into numpy array\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "x_train, y_train = splitSequence(train_set, input_sequence_length)\n",
    "x_val, y_val = splitSequence(val_set, input_sequence_length)\n",
    "x_test, y_test = splitSequence(test_set, input_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = data.shape[1]\n",
    "units = 64\n",
    "output_size = data.shape[1]\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(InputLayer((input_sequence_length, input_dim)))\n",
    "#return sequences is necessary for sequential LSTM layers\n",
    "model.add(LSTM(units, return_sequences=True))\n",
    "model.add(LSTM(units))\n",
    "model.add(Dense(output_size))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    #makes the training stop early if it notices no improvements on the validation set 10 times in a row, to prevent overfitting\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(test_dataset)\n",
    "print(np.min(results))\n",
    "#takes labels from the test_dataset\n",
    "y = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "#since each result is an array of each predicted sensor you can input 0 through 9 to look at the graph\n",
    "plt.plot(results[:, 7], label='predicted')\n",
    "#if you are wondering why it is like this look at y.shape\n",
    "plt.plot(y[:,0,7], label='should')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
