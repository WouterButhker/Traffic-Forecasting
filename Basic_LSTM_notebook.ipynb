{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "df = pd.read_csv('data/K120.csv', delimiter=';').to_numpy()\n",
    "data = np.array(df[:, 1:], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/drive/1b3CUJuDOmPmNdZFH3LQDmt5F0K3FZhqD?usp=sharing#scrollTo=bY2yEu2QTBXP\n",
    "def df_to_X_y(data, window_size=5):\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(data)-window_size):\n",
    "    row = [[a] for a in data[i:i+window_size]]\n",
    "    X.append(row)\n",
    "    label = data[i+window_size]\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)\n",
    "\n",
    "#split and normalize the data\n",
    "window_size = 10\n",
    "X, Y = df_to_X_y(data, window_size=window_size)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_steps = data.shape[0]\n",
    "train_size = 0.5\n",
    "val_size = 0.2\n",
    "\n",
    "num_train = int(num_of_steps * train_size)\n",
    "num_val = int(num_of_steps * val_size)\n",
    "\n",
    "train_set = data[:num_train]\n",
    "mean, std = np.nanmean(train_set, axis=0), np.nanstd(train_set, axis=0)\n",
    "\n",
    "train_set = (train_set - mean) / std\n",
    "val_set = (data[num_train:num_train + num_val] - mean) / std\n",
    "test_set = (data[num_train + num_val:] - mean) / std\n",
    "\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import timeseries_dataset_from_array\n",
    "\n",
    "#inputs series of sequential inputs\n",
    "\n",
    "\n",
    "#targets, depending on the forecast horizon and if we want to return multiples a\n",
    "\n",
    "batch_size = 32\n",
    "input_sequence_length = 8\n",
    "forecast_horizon = 1\n",
    "multi_horizon = False\n",
    "\n",
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    target_offset = (\n",
    "        input_sequence_length\n",
    "        if multi_horizon\n",
    "        else input_sequence_length + forecast_horizon - 1\n",
    "    )\n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        data_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "train_dataset, val_dataset = (\n",
    "    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
    "    for data_array in [train_set, val_set]\n",
    ")\n",
    "\n",
    "test_dataset = create_tf_dataset(\n",
    "    test_set,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=test_set.shape[0],\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "\n",
    "batch_size = 32\n",
    "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
    "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
    "input_dim = data.shape[1]\n",
    "\n",
    "units = 64\n",
    "output_size = data.shape[1]  # labels are from 0 to 9\n",
    "\n",
    "# Build the RNN model\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    if allow_cudnn_kernel:\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
    "    else:\n",
    "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "        lstm_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
    "        )\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            keras.layers.Dense(output_size),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "model = build_model()\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=MeanSquaredError(),\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=500,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
