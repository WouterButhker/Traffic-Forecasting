{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data_cars/'\n",
    "all_files = os.listdir(DATA_PATH)\n",
    "\n",
    "all_dataframes = []\n",
    "for index, file in enumerate(all_files):\n",
    "    print(f\"Reading file: {file}\")\n",
    "    file_name = file.split('.')[0]\n",
    "    df = pd.read_csv(DATA_PATH + file, sep=';')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df[file_name], format='%Y-%m-%d %H:%M')\n",
    "    df = df.drop(columns=[file_name])\n",
    "\n",
    "    df = df.set_index('date')\n",
    "    df.columns = [f\"{file_name}_{col}\" for col in df.columns if col != 'date']\n",
    "    all_dataframes.append(df)\n",
    "    print(f\"Finished reading file: {file}, shape = {df.shape}\")\n",
    "\n",
    "combined_df = pd.concat(all_dataframes, axis=1)\n",
    "combined_df.fillna(method='ffill', inplace=True)\n",
    "combined_df['hour'] = combined_df.index.hour\n",
    "combined_df['day_of_week'] = combined_df.index.dayofweek\n",
    "\n",
    "data = np.array(combined_df, dtype=float)[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def getMostCorrelated(sensor, no_extraSensors, data):\n",
    "    corr = np.corrcoef(data.T)\n",
    "    best = np.argsort(corr[sensor])[-no_extraSensors:]\n",
    "    return data[:, best]\n",
    "\n",
    "def getScores(model, x, y, scaler):\n",
    "    predicts = model.predict(x)\n",
    "    scaled_predict, y_scaled = scaler.inverse_transform(predicts), scaler.inverse_transform(y)\n",
    "    rmse =  np.sqrt(mean_squared_error(y_scaled, scaled_predict))\n",
    "    mae = mean_absolute_error(y_scaled, scaled_predict)\n",
    "    return rmse, mae\n",
    "\n",
    "def getScoresConv(model, x, y, scaler):\n",
    "    predicts = model.predict(x)\n",
    "    scaled_predict, y_scaled = scaler.inverse_transform(predicts.reshape(predicts.shape[0], predicts.shape[1])), scaler.inverse_transform(y)\n",
    "    rmse =  np.sqrt(mean_squared_error(y_scaled[:, -1], scaled_predict[:, -1]))\n",
    "    mae = mean_absolute_error(y_scaled[:, -1], scaled_predict[:, -1])\n",
    "    return rmse, mae\n",
    "\n",
    "\n",
    "def splitSequence(seq, n_steps):\n",
    "\n",
    "    #Declare X and y as empty list\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        #get the last index\n",
    "        lastIndex = i + n_steps\n",
    "\n",
    "        #if lastIndex is greater than length of sequence then break\n",
    "        if lastIndex > len(seq) - 1:\n",
    "            break\n",
    "\n",
    "        # Create input and output sequence\n",
    "        # Last 2 columns are time of day and day of week\n",
    "        seq_X, seq_y = seq[i:lastIndex], seq[lastIndex]\n",
    "\n",
    "        #append seq_X, seq_y in X and y list\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        #Convert X and y into numpy array\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def shiftSequence(seq, n_steps):\n",
    "\n",
    "    #Declare X and y as empty list\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        #get the last index\n",
    "        lastIndex = i + n_steps\n",
    "\n",
    "        #if lastIndex is greater than length of sequence then break\n",
    "        if lastIndex > len(seq) - 1:\n",
    "            break\n",
    "\n",
    "        # Create input and output sequence\n",
    "        # Last 2 columns are time of day and day of week\n",
    "        seq_X, seq_y = seq[i:lastIndex], seq[i+1:lastIndex+1]\n",
    "\n",
    "        #append seq_X, seq_y in X and y list\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        #Convert X and y into numpy array\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "#Following structure layed out in github.com/locuslab/TCN\n",
    "from keras.models import Sequential\n",
    "def createTemporalConvNetwork(n_layers, n_sensors, look_back, n_outputs, kernel_size=2, dropout=0.2):\n",
    "    modelTCN = Sequential()\n",
    "    modelTCN.add(layers.InputLayer((look_back, n_sensors)))\n",
    "    for i in range(n_layers-1):\n",
    "        modelTCN.add(layers.Conv1D(n_outputs[i], kernel_size=kernel_size, padding='causal', activation='relu', dilation_rate=2**i))\n",
    "        modelTCN.add(layers.Dropout(dropout))\n",
    "        modelTCN.add(layers.Conv1D(n_outputs[i], kernel_size=kernel_size, padding='causal', activation='relu', dilation_rate=2**i))\n",
    "        modelTCN.add(layers.Dropout(dropout))\n",
    "    modelTCN.add(layers.Conv1D(n_outputs[-1], kernel_size=kernel_size, padding='causal', activation='relu', dilation_rate=2**(n_layers-1)))\n",
    "    return modelTCN\n",
    "\n",
    "#Following structure layed out in github.com/locuslab/TCN\n",
    "def createLSTMNetwork(n_sensors, look_back, n_outputs):\n",
    "    modelLSTM = Sequential()\n",
    "    modelLSTM.add(layers.InputLayer((look_back, n_sensors)))\n",
    "    modelLSTM.add(layers.LSTM(128, return_sequences=True))\n",
    "    modelLSTM.add(layers.LSTM(128))\n",
    "    modelLSTM.add(layers.Dense(n_outputs))\n",
    "    return modelLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentTCN(diff_inputs, samples, averaging, look_back):\n",
    "    num_of_steps = data.shape[0] - look_back\n",
    "    train_size = 0.6\n",
    "    val_size = 0.15\n",
    "    num_train = int(num_of_steps * train_size)\n",
    "    num_val = int(num_of_steps * val_size)\n",
    "\n",
    "    RMSE_scores = np.zeros((len(samples), len(diff_inputs)))\n",
    "    MAE_scores = np.zeros((len(samples), len(diff_inputs)))\n",
    "    for sensId, sensor in enumerate(samples):\n",
    "        \n",
    "        _, y = shiftSequence(data[:, sensor], look_back)\n",
    "        y_scaler = MinMaxScaler()\n",
    "        y = y_scaler.fit_transform(y)\n",
    "        y_train, y_val, y_test = y[:num_train], y[num_train:num_train+num_val], y[num_train+num_val:]\n",
    "\n",
    "        for idx, inputs in enumerate(diff_inputs):\n",
    "            correlated_data = getMostCorrelated(sensor, inputs, data)\n",
    "            scaler = MinMaxScaler()\n",
    "            correlated_data = scaler.fit_transform(correlated_data)\n",
    "            x, _ = shiftSequence(correlated_data, look_back)\n",
    "            x_train, x_val, x_test = x[:num_train], x[num_train:num_train+num_val], x[num_train+num_val:]\n",
    "            \n",
    "            scores_rmse = np.zeros(averaging)\n",
    "            scores_mae = np.zeros(averaging)\n",
    "            for av in range(averaging):\n",
    "                outputs = [64, 128, 128, 128, 64, 1]\n",
    "                model = createTemporalConvNetwork(6, inputs, look_back, outputs, kernel_size=3)\n",
    "                model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[keras.metrics.RootMeanSquaredError()],)\n",
    "                cb = [keras.callbacks.EarlyStopping(patience=5)]\n",
    "                model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=32, epochs=500, callbacks=cb)\n",
    "                rmse, mae = getScoresConv(model, x_test, y_test, y_scaler)\n",
    "                print(f'{rmse:.2f} RMSE')\n",
    "                scores_rmse[av] = rmse\n",
    "                scores_mae[av] = mae\n",
    "            RMSE_scores[sensId, idx] = np.average(scores_rmse)\n",
    "            MAE_scores[sensId, idx] = np.average(scores_mae)\n",
    "    return RMSE_scores, MAE_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentLSTM(diff_inputs, samples, averaging, look_back):\n",
    "    num_of_steps = data.shape[0] - look_back\n",
    "    train_size = 0.6\n",
    "    val_size = 0.15\n",
    "    num_train = int(num_of_steps * train_size)\n",
    "    num_val = int(num_of_steps * val_size)\n",
    "\n",
    "    RMSE_scores = np.zeros((len(samples), len(diff_inputs)))\n",
    "    MAE_scores = np.zeros((len(samples), len(diff_inputs)))\n",
    "    for sensId, sensor in enumerate(samples):\n",
    "        \n",
    "        _, y = splitSequence(data[:, sensor], look_back)\n",
    "        y_scaler = StandardScaler()\n",
    "        y = y_scaler.fit_transform(y.reshape(-1, 1))\n",
    "        y_train, y_val, y_test = y[:num_train], y[num_train:num_train+num_val], y[num_train+num_val:]\n",
    "\n",
    "        for idx, inputs in enumerate(diff_inputs):\n",
    "            correlated_data = getMostCorrelated(sensor, inputs, data)\n",
    "            scaler = StandardScaler()\n",
    "            correlated_data = scaler.fit_transform(correlated_data)\n",
    "            x, _ = splitSequence(correlated_data, look_back)\n",
    "            x_train, x_val, x_test = x[:num_train], x[num_train:num_train+num_val], x[num_train+num_val:]\n",
    "            \n",
    "            scores_rmse = np.zeros(averaging)\n",
    "            scores_mae = np.zeros(averaging)\n",
    "            for av in range(averaging):\n",
    "                model = createLSTMNetwork(inputs, look_back, 1)\n",
    "                model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[keras.metrics.RootMeanSquaredError()],)\n",
    "                cb = [keras.callbacks.EarlyStopping(patience=5)]\n",
    "                model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=32, epochs=500, callbacks=cb)\n",
    "                rmse, mae = getScores(model, x_test, y_test, y_scaler)\n",
    "                print(f'{rmse:.2f} RMSE')\n",
    "                scores_rmse[av] = rmse\n",
    "                scores_mae[av] = mae\n",
    "            RMSE_scores[sensId, idx] = np.average(scores_rmse)\n",
    "            MAE_scores[sensId, idx] = np.average(scores_mae)\n",
    "    return RMSE_scores, MAE_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [0, 23, 78]\n",
    "inputs = [14, 16, 18, 20]\n",
    "rmse_tcn, mae_tcn = experimentTCN(inputs, outputs, 10, 96)\n",
    "rmse_lstm, mae_lstm = experimentLSTM(inputs, outputs, 10, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tcn, mae_tcn, rmse_lstm, mae_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [0, 23, 78]\n",
    "inputs = [1, 2, 4, 6, 8, 10, 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 96\n",
    "num_of_steps = data.shape[0] - look_back\n",
    "train_size = 0.6\n",
    "val_size = 0.15\n",
    "num_train = int(num_of_steps * train_size)\n",
    "num_val = int(num_of_steps * val_size)\n",
    "\n",
    "sensor = 0\n",
    "_, y = splitSequence(data[:, sensor], look_back)\n",
    "y_scaler = StandardScaler()\n",
    "y = y_scaler.fit_transform(y.reshape(-1, 1))\n",
    "y_train, y_val, y_test = y[:num_train], y[num_train:num_train+num_val], y[num_train+num_val:]\n",
    "\n",
    "correlated_data = getMostCorrelated(sensor, 1, data)\n",
    "scaler = StandardScaler()\n",
    "correlated_data = scaler.fit_transform(correlated_data)\n",
    "x, _ = splitSequence(correlated_data, look_back)\n",
    "x_train, x_val, x_test = x[:num_train], x[num_train:num_train+num_val], x[num_train+num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(model, x, y, scaler):\n",
    "    predicts = model.predict(x)\n",
    "    scaled_predict, y_scaled = scaler.inverse_transform(predicts), scaler.inverse_transform(y)\n",
    "    rmse =  np.sqrt(mean_squared_error(y_scaled, scaled_predict))\n",
    "    mae = mean_absolute_error(y_scaled, scaled_predict)\n",
    "    return rmse, mae\n",
    "\n",
    "def getScoresConv(model, x, y, scaler):\n",
    "    predicts = model.predict(x)\n",
    "    scaled_predict, y_scaled = scaler.inverse_transform(predicts[:, -1]), scaler.inverse_transform(y[:, -1])\n",
    "    rmse =  np.sqrt(mean_squared_error(y_scaled, scaled_predict))\n",
    "    mae = mean_absolute_error(y_scaled, scaled_predict)\n",
    "    return rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentTCN_output(outputs, averaging, look_back):\n",
    "        num_of_steps = data.shape[0] - look_back\n",
    "        train_size = 0.6\n",
    "        val_size = 0.15\n",
    "        num_train = int(num_of_steps * train_size)\n",
    "        num_val = int(num_of_steps * val_size)\n",
    "\n",
    "        RMSE_scores = np.zeros(len(outputs))\n",
    "        MAE_scores = np.zeros(len(outputs))\n",
    "\n",
    "        x, _ = shiftSequence(data, look_back)\n",
    "        x_train, x_val, x_test = x[:num_train], x[num_train:num_train+num_val], x[num_train+num_val:]\n",
    "    \n",
    "\n",
    "        for idx, output in enumerate(outputs):\n",
    "                \n",
    "                y_scaler = MinMaxScaler()\n",
    "                y = y_scaler.fit_transform(data[:, :output])\n",
    "                _, y = shiftSequence(y, look_back)\n",
    "                y_train, y_val, y_test = y[:num_train], y[num_train:num_train+num_val], y[num_train+num_val:]\n",
    "\n",
    "                scores_rmse = np.zeros(averaging)\n",
    "                scores_mae = np.zeros(averaging)\n",
    "                for av in range(averaging):\n",
    "                        outputs = [64, 128, 128, 128, 64, output]\n",
    "                        model = createTemporalConvNetwork(6, data.shape[1], look_back, outputs, kernel_size=3)\n",
    "                        model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[keras.metrics.RootMeanSquaredError()],)\n",
    "                        cb = [keras.callbacks.EarlyStopping(patience=10)]\n",
    "                        model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), batch_size=32, epochs=500, callbacks=cb)\n",
    "                        rmse, mae = getScoresConv(model, x_test, y_test, y_scaler)\n",
    "                        print(f'{rmse:.2f} RMSE')\n",
    "                        scores_rmse[av] = rmse\n",
    "                        scores_mae[av] = mae\n",
    "                RMSE_scores[idx] = np.average(scores_rmse)\n",
    "                MAE_scores[idx] = np.average(scores_mae)\n",
    "        return RMSE_scores, MAE_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [1]\n",
    "rmse_tcn, mae_tcn = experimentTCN_output(outputs, 10, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
